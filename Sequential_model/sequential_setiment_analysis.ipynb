{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33176\\2922694869.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m from pandas.compat.numpy import (\n\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10\\lib\\site-packages\\pandas\\_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;31m# array-like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ExtensionArray\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mAnyArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Series\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "    ## Sequential model\n",
    "    #Trained on IMDB dataset \n",
    "    #https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense\n",
    "from keras.regularizers import l1\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "\n",
    "max_words = 10000  # Maximum number of unique words to consider\n",
    "max_sequence_length = 250  # Maximum length of each\n",
    "embedding_dim = 300  # Dimension of the GloVe word embeddings\n",
    "negation_words = ['not', 'no', 'never']  # Words to be considered as negation words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def sentiment_to_binary(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text_with_negation(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Preserve negation words by appending an underscore to the following word\n",
    "    negation_words = ['not', 'no', 'never']\n",
    "    tokens = [tokens[i] + \"_\" + tokens[i+1] if tokens[i] in negation_words else tokens[i] for i in range(len(tokens) - 1)] + [tokens[-1]]\n",
    "    \n",
    "    # Remove non-alphanumeric characters (keeping the underscore for negation)\n",
    "    tokens = [re.sub(r'[^a-z0-9_]', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove any empty strings that may result from the previous step\n",
    "    tokens = [token for token in tokens if token]\n",
    "    \n",
    "    # Combine the tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 300)          36865200  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 250, 300)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 250, 128)          115328    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 125, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 125, 200)         183200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 125, 100)         100400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 125, 25)          2525      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 25)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                650       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,267,329\n",
      "Trainable params: 402,129\n",
      "Non-trainable params: 36,865,200\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../IMDB Dataset.csv')\n",
    "\n",
    "# Apply the preprocess_text_with_negation function to the 'review' column\n",
    "df['review'] = df['review'].apply(preprocess_text_with_negation)\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(sentiment_to_binary)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_embeddings('../glove.42B.300d.txt')\n",
    "embedding_matrix = create_embedding_matrix(embeddings_index, word_index, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Adding a 1D convolutional layer\n",
    "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.0, recurrent_dropout=0.0, kernel_regularizer=l1(0.001), return_sequences=True)))\n",
    "\n",
    "# Adding a second LSTM layer\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.0, recurrent_dropout=0.0, kernel_regularizer=l1(0.001), return_sequences=True)))\n",
    "\n",
    "# Adding a TimeDistributed layer with a dense output\n",
    "model.add(TimeDistributed(Dense(25, activation='relu')))\n",
    "\n",
    "# Adding a GlobalMaxPooling1D layer to reduce the output to a single vector\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Adding a dense layer before the final output layer\n",
    "model.add(Dense(25, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "# Create a checkpoint callback\n",
    "# checkpoint = ModelCheckpoint('sentiment_analysis_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "print('Training the model...')\n",
    "with open('new_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "# Save the trained model\n",
    "model.save('sentiment_analysis_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step\n",
      "0.30709106\n",
      "The sentiment of the statement \"fantastic movie \" is negative.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('../sentiment_analysis_model.h5')\n",
    "\n",
    "# Maximum length of each sequence\n",
    "max_sequence_length = 250\n",
    "negation_words = ['not', 'no', 'never']  # Words to be considered as negation words\n",
    "\n",
    "# Words to be considered as negation words\n",
    "def preprocess_text_with_negation(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace any negation words with \"not_\" to preserve their meaning in the model\n",
    "    # Remove any non-alphanumeric characters\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to convert user input to a padded sequence of fixed length\n",
    "def preprocess_input(input_text, tokenizer):\n",
    "    input_text = preprocess_text_with_negation(input_text)\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "    return input_padded\n",
    "\n",
    "# Function to predict the sentiment of the user input\n",
    "def predict_sentiment(input_text, tokenizer, model):\n",
    "    input_padded = preprocess_input(input_text, tokenizer)\n",
    "    prediction = model.predict(input_padded)[0][0]\n",
    "    print(prediction)\n",
    "    if prediction >= 0.5:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# Load the Tokenizer used during training\n",
    "tokenizer = pickle.load(open('../new_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# Get user input\n",
    "user_input = input('Enter a statement: ')\n",
    "    \n",
    "# Predict the sentiment of the user input\n",
    "prediction = predict_sentiment(user_input, tokenizer, model)\n",
    "\n",
    "print(f'The sentiment of the statement \"{user_input}\" is {prediction}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_good good moive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i absolutely loved this movie the plot was eng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what a disappointment the storyline was weak a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the cinematography in this film was breathtaki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i struggled to stay awake during this movie th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0                                not_good good moive          0\n",
       "1  i absolutely loved this movie the plot was eng...          1\n",
       "2  what a disappointment the storyline was weak a...          0\n",
       "3  the cinematography in this film was breathtaki...          1\n",
       "4  i struggled to stay awake during this movie th...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

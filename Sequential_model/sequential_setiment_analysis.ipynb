{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "    ## Sequential model\n",
    "    #Trained on IMDB dataset \n",
    "    #https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense\n",
    "from keras.regularizers import l1\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "\n",
    "max_words = 15000  # Maximum number of unique words to consider\n",
    "max_sequence_length = 250  # Maximum length of each\n",
    "embedding_dim = 300  # Dimension of the GloVe word embeddings\n",
    "negation_words = ['not', 'no', 'never']  # Words to be considered as negation words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def sentiment_to_binary(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text_with_negation(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (keeping the underscore for negation)\n",
    "    tokens = [re.sub(r'[^a-z0-9_]', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove any empty strings that may result from the previous step\n",
    "    tokens = [token for token in tokens if token]\n",
    "    \n",
    "    # Combine the tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 300)          48029700  \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 250, 300)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 250, 200)         320800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 250, 25)          5025      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 25)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 25)                650       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,356,201\n",
      "Trainable params: 48,356,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training the model...\n",
      "Epoch 1/5\n",
      "627/627 [==============================] - 35s 52ms/step - loss: 1.0933 - accuracy: 0.8036 - val_loss: 0.3338 - val_accuracy: 0.9021\n",
      "Epoch 2/5\n",
      "627/627 [==============================] - 34s 54ms/step - loss: 0.3127 - accuracy: 0.9123 - val_loss: 0.3155 - val_accuracy: 0.9060\n",
      "Epoch 3/5\n",
      "627/627 [==============================] - 35s 56ms/step - loss: 0.2502 - accuracy: 0.9368 - val_loss: 0.3599 - val_accuracy: 0.8880\n",
      "Epoch 4/5\n",
      "627/627 [==============================] - 35s 56ms/step - loss: 0.2069 - accuracy: 0.9540 - val_loss: 0.3443 - val_accuracy: 0.9032\n",
      "Epoch 5/5\n",
      "627/627 [==============================] - 36s 57ms/step - loss: 0.1812 - accuracy: 0.9646 - val_loss: 0.4542 - val_accuracy: 0.8793\n",
      "Accuracy: 87.93%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../IMDB Dataset.csv')\n",
    "\n",
    "# Apply the preprocess_text_with_negation function to the 'review' column\n",
    "df['review'] = df['review'].apply(preprocess_text_with_negation)\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(sentiment_to_binary)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert the text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_embeddings('../glove.42B.300d.txt')\n",
    "embedding_matrix = create_embedding_matrix(embeddings_index, word_index, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Adding a 1D convolutional layer\n",
    "# model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.0, recurrent_dropout=0.0, kernel_regularizer=l1(0.001), return_sequences=True)))\n",
    "\n",
    "# Adding a TimeDistributed layer to apply a dense layer to each of the outputs of the LSTM layer\n",
    "    \n",
    "\n",
    "model.add(TimeDistributed(Dense(25, activation='relu')))\n",
    "\n",
    "# Adding a GlobalMaxPooling1D layer to reduce the output to a single vector\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Adding a dense layer before the final output layer\n",
    "model.add(Dense(25, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "# Create a checkpoint callback\n",
    "# checkpoint = ModelCheckpoint('sentiment_analysis_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "print('Training the model...')\n",
    "with open('new_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# model = load_model('sentiment_analysis_model.h5')\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "# Save the trained model\n",
    "model.save('sentiment_analysis_model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "0.6869869\n",
      "The sentiment of the statement \"pussy\" is positive.\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "0.5724936\n",
      "The sentiment of the statement \"good\" is positive.\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "0.70202893\n",
      "The sentiment of the statement \"dick\" is positive.\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "0.6869869\n",
      "The sentiment of the statement \"dickhead\" is positive.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('../sentiment_analysis_model_checkpoint03.h5')\n",
    "\n",
    "# Maximum length of each sequence\n",
    "max_sequence_length = 250\n",
    "negation_words = ['not', 'no', 'never']  # Words to be considered as negation words\n",
    "\n",
    "# Words to be considered as negation words\n",
    "def preprocess_text_with_negation(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace any negation words with \"not_\" to preserve their meaning in the model\n",
    "    # Remove any non-alphanumeric characters\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to convert user input to a padded sequence of fixed length\n",
    "def preprocess_input(input_text, tokenizer):\n",
    "    input_text = preprocess_text_with_negation(input_text)\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "    return input_padded\n",
    "\n",
    "# Function to predict the sentiment of the user input\n",
    "def predict_sentiment(input_text, tokenizer, model):\n",
    "    input_padded = preprocess_input(input_text, tokenizer)\n",
    "    prediction = model.predict(input_padded)[0][0]\n",
    "    print(prediction)\n",
    "    if prediction >= 0.5:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# Load the Tokenizer used during training\n",
    "tokenizer = pickle.load(open('../new_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# Get user input\n",
    "user_input = input('Enter a statement: ')\n",
    "\n",
    "#while loop to keep asking for input\n",
    "while user_input != 'exit':\n",
    "    # Predict the sentiment of the user input\n",
    "    prediction = predict_sentiment(user_input, tokenizer, model)\n",
    "\n",
    "    print(f'The sentiment of the statement \"{user_input}\" is {prediction}.')\n",
    "    user_input = input('Enter a statement: ')\n",
    "    \n",
    "# # Predict the sentiment of the user input\n",
    "# prediction = predict_sentiment(user_input, tokenizer, model)\n",
    "\n",
    "# print(f'The sentiment of the statement \"{user_input}\" is {prediction}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23268/2573261369.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../sentiment_analysis_model2.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheckpoint_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sentiment_analysis_model_checkpoint{epoch:02d}.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('../sentiment_analysis_model2.h5')\n",
    "\n",
    "checkpoint_filepath = 'sentiment_analysis_model_checkpoint{epoch:02d}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test), callbacks=[checkpoint])\n",
    "\n",
    "model.save('sentiment_analysis_model2.1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

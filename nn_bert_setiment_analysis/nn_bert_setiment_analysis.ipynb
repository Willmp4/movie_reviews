{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense\n",
    "from keras.regularizers import l1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_words = 10000  # Maximum number of unique words to consider\n",
    "max_sequence_length = 250  # Maximum length of each\n",
    "embedding_dim = 300  # Dimension of the GloVe word embeddings\n",
    "negation_words = ['not', 'no', 'never']  # Words to be considered as negation words\n",
    "accumulation_steps = 4\n",
    "learning_rates = [2e-5, 5e-5, 1e-4]\n",
    "patience = 2  # Set the number of epochs to wait before stopping if validation loss doesn't decrease\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def create_embedding_matrix(embeddings_index, word_index, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def sentiment_to_binary(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def preprocess_text_with_negation(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace any negation words with \"not_\" to preserve their meaning in the model\n",
    "    for negation_word in negation_words:\n",
    "        text = text.replace(negation_word, \"not_\" + negation_word)\n",
    "    \n",
    "    # Remove any non-alphanumeric characters\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epochs:   0%|                                                                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                | 0/20 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"intermediate\" \"                 f\"(type TFBertIntermediate).\n\n{{function_node __wrapped__Erf_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Erf]\n\nCall arguments received by layer \"intermediate\" \"                 f\"(type TFBertIntermediate):\n  • hidden_states=tf.Tensor(shape=(8, 250, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3992/4281607772.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)\u001b[0m\n\u001b[0;32m   1451\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output_hidden_states\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1452\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1453\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1454\u001b[0m         )\n\u001b[0;32m   1455\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output_hidden_states\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m         )\n\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    436\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m                 \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             )\n\u001b[0;32m    440\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m         layer_output = self.bert_output(\n\u001b[0;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wgoud\\.conda\\envs\\tf2.10_\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"intermediate\" \"                 f\"(type TFBertIntermediate).\n\n{{function_node __wrapped__Erf_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Erf]\n\nCall arguments received by layer \"intermediate\" \"                 f\"(type TFBertIntermediate):\n  • hidden_states=tf.Tensor(shape=(8, 250, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../IMDB Dataset.csv')\n",
    "\n",
    "# Apply the preprocess_text_with_negation function to the 'review' column\n",
    "df['review'] = df['review'].apply(preprocess_text_with_negation)\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(sentiment_to_binary)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the text\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=250)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=250)\n",
    "\n",
    "# Convert the tokenized data into a TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(1000).batch(8)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(8)\n",
    "\n",
    "# # Fine-tune the model\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# print('Training the model...')\n",
    "# model.fit(train_dataset, epochs=3, batch_size=4, validation_data=test_dataset)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred_logits = model.predict(test_dataset, batch_size=4)\n",
    "# y_pred = np.argmax(y_pred_logits.logits, axis=1)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "epochs_without_improvement = 0\n",
    "patience = 2\n",
    "\n",
    "for epoch in tqdm.tqdm(range(20), desc=\"Epochs\", ncols=100):  # Increase the maximum number of epochs since we're using early stopping\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "\n",
    "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True).logits\n",
    "            batch_loss = loss(y_batch, logits)\n",
    "\n",
    "        gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "        accumulated_gradients = [(acc_grad + grad) for acc_grad, grad in zip(accumulated_gradients, gradients)]\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
    "            accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "\n",
    "        epoch_loss += batch_loss.numpy()\n",
    "        epoch_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(f\"  Step {step}, loss: {batch_loss.numpy()}\")\n",
    "\n",
    "    epoch_loss /= epoch_steps\n",
    "    print(f\"  Epoch loss: {epoch_loss}\")\n",
    "\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        logits = model(x_batch, training=False).logits\n",
    "        batch_loss = loss(y_batch, logits)\n",
    "        val_loss += batch_loss.numpy()\n",
    "        val_steps += 1\n",
    "    val_loss /= val_steps\n",
    "    print(f\"  Validation loss: {val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save the best model\n",
    "        model.save_pretrained('sentiment_analysis_bert_/')\n",
    "        tokenizer.save_pretrained('sentiment_analysis_bert_/')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('sentiment_analysis_bert_/')\n",
    "tokenizer.save_pretrained('sentiment_analysis_bert_/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at sentiment_analysis_bert were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at sentiment_analysis_bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "Predicted sentiment: 1\n",
      "Negative probability: 0.002638840349391103\n",
      "Positive probability: 0.9973611235618591\n",
      "not good\n",
      "Predicted sentiment: 0\n",
      "Negative probability: 0.9988136291503906\n",
      "Positive probability: 0.0011864054249599576\n",
      "bad\n",
      "Predicted sentiment: 0\n",
      "Negative probability: 0.9990054965019226\n",
      "Positive probability: 0.0009945285273715854\n",
      "not bad\n",
      "Predicted sentiment: 0\n",
      "Negative probability: 0.9989626407623291\n",
      "Positive probability: 0.001037338632158935\n",
      "not great\n",
      "Predicted sentiment: 1\n",
      "Negative probability: 0.05264824256300926\n",
      "Positive probability: 0.9473516941070557\n",
      "great\n",
      "Predicted sentiment: 1\n",
      "Negative probability: 0.001551531720906496\n",
      "Positive probability: 0.9984484910964966\n",
      "t\n",
      "Predicted sentiment: 0\n",
      "Negative probability: 0.7120198607444763\n",
      "Positive probability: 0.2879801094532013\n",
      "pussy\n",
      "Predicted sentiment: 0\n",
      "Negative probability: 0.8933406472206116\n",
      "Positive probability: 0.10665930062532425\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_path = 'sentiment_analysis_bert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Perform the sentiment prediction\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Convert the logits to probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "\n",
    "    # Get the predicted class (0: Negative, 1: Positive)\n",
    "    predicted_class = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# while loop to take input from user\n",
    "while True:\n",
    "    # Take input from user\n",
    "    text = input(\"Enter a sentence to predict its sentiment (or 'q' to quit): \")\n",
    "    \n",
    "    # Check if the user wants to quit\n",
    "    if text == 'q':\n",
    "        break\n",
    "    \n",
    "    # Predict the sentiment\n",
    "    predicted_class, probabilities = predict_sentiment(text)\n",
    "    \n",
    "    # Print the results\n",
    "    print(text)\n",
    "    print(\"Predicted sentiment: {}\".format(predicted_class[0]))\n",
    "    print(\"Negative probability: {}\".format(probabilities[0][0]))\n",
    "    print(\"Positive probability: {}\".format(probabilities[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_209']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE, Score: 1.00\n",
      "Sentiment: POSITIVE, Score: 0.75\n",
      "Sentiment: POSITIVE, Score: 0.75\n",
      "Sentiment: NEGATIVE, Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Loop to take user input and get the sentiment analysis output\n",
    "while True:\n",
    "    user_input = input(\"Enter a sentence: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        result = nlp(user_input)\n",
    "        print(f\"Sentiment: {result[0]['label']}, Score: {result[0]['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
